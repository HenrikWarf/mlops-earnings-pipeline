name: Model evaluation
inputs:
- {name: MODEL, type: Model}
- {name: TEST_DATA, type: Dataset}
- {name: num_of_examples, type: Integer}
- {name: categorical_col, type: Integer}
- {name: numeric_col, type: Integer}
- {name: pipeline, type: String}
- {name: framework, type: String}
- {name: input_path, type: String}
- {name: dataset_version, type: Integer}
outputs:
- {name: pipeline_metrics, type: Metrics}
- {name: accuracy, type: Float}
- {name: loss, type: Float}
- {name: dep_decision, type: String}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'google-cloud-bigquery' 'pyarrow' 'gcsfs' 'tensorflow==2.5' 'google-cloud-aiplatform' 'db_dtypes' 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef model_evaluation(MODEL : Input[Model], \n                \
      \            TEST_DATA: Input[Dataset], \n                            num_of_examples:\
      \ int,\n                            categorical_col: int,\n                \
      \            numeric_col: int,\n                            pipeline:str, \n\
      \                            framework:str,\n                            input_path:str,\n\
      \                            dataset_version:int,\n                        \
      \    pipeline_metrics: Output[Metrics]) ->  NamedTuple(\n                  \
      \                                                  'ComponentOutputs',\n   \
      \                                                                          \
      \ [\n                                                                      \
      \            ('accuracy', float),\n                                        \
      \                                          ('loss', float),\n              \
      \                                                                    ('dep_decision',str)\n\
      \                                                                          \
      \    ]):\n\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow\
      \ import keras\n\n    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n    def\
      \ helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n      \
      \  dataframe = dataframe.copy()\n        labels = dataframe.pop('income_bracket')\n\
      \        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n\
      \        if shuffle:\n            ds = ds.shuffle(buffer_size=len(dataframe))\n\
      \        ds = ds.batch(batch_size)\n        ds = ds.prefetch(batch_size)\n \
      \       return ds\n\n    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n    reloaded_model\
      \ = tf.keras.models.load_model(MODEL.path)\n\n    #READ TESTING DATASET\n  \
      \  TESTING_DATA = pd.read_csv(TEST_DATA.path)\n\n    #SET BATCG SIZE\n    BATCH_SIZE\
      \ = 32\n\n    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n    TESTING_DS\
      \ = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n\
      \n    #EVALUATE MODEL WITH TEST DATA\n    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n\
      \n    accuracy = float(accuracy)\n    loss = float(loss)\n    dep_decision =\
      \ 'false'\n\n    #PRINT ACCURACY METRIC\n    print(\"Accuracy\", accuracy)\n\
      \    print(\"Loss\", loss)\n\n\n    from tensorflow.python.lib.io import file_io\
      \    \n\n    #Write Metrics to BigQuery Table for Validation and possible promotion\
      \ to Deployment\n    from google.cloud.bigquery import Client, QueryJobConfig\n\
      \n    #Initiate BigQuery Client\n    client = Client(project='crazy-hippo-01')\n\
      \n    print('Sending Metrics into BigQuery')\n\n    #Define DML Query to Insert\
      \ Metrics into BugQuery\n    query = \"\"\"INSERT `crazy-hippo-01.census_data_us.model_metrics_history`\
      \ (model_name, pipeline, framework, accuracy, loss)\n    VALUES (\"{}\", \"\
      {}\", \"{}\", {}, {})  \n    \"\"\".format(MODEL.path, pipeline, framework,\
      \ accuracy, loss)\n\n    #Run Query\n    job = client.query(query)\n\n    print('Metrics\
      \ sent to BigQuery!')\n\n    # Export two metrics\n    pipeline_metrics.log_metric('accuracy',\
      \ accuracy)\n    pipeline_metrics.log_metric('loss', loss)\n\n    from collections\
      \ import namedtuple\n\n    component_outputs = namedtuple('ComponentOutputs',\n\
      \        ['accuracy', 'loss'])\n\n\n    from datetime import datetime\n\n  \
      \  TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    MY_PROJECT = 'crazy-hippo-01'\n\
      \    REGION = 'us-central1'\n    EXPERIMENT_NAME = 'earnings-classifier-ver1'\n\
      \    RUN_NAME = \"tensorflow-dl-model-\" + TIMESTAMP\n\n\n    #Store Experiment\
      \ Metrics in Vertex AI\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=MY_PROJECT,\
      \ location=REGION, experiment=EXPERIMENT_NAME)\n    aiplatform.start_run(run=RUN_NAME)\n\
      \n    PARAMETERS = {\n        #\"Model\" : MODEL,\n        #\"Pipeline\" : pipeline,\n\
      \        #\"Dataset\" : input_path,\n        \"Dataset Version\" : dataset_version\n\
      \    }\n\n    aiplatform.log_params(PARAMETERS)\n\n    METRICS = {\n       \
      \ 'Num_of_examples': num_of_examples,\n        'Categorical_col': categorical_col,\n\
      \        'Numeric_col': numeric_col,\n        \"Accuracy\": accuracy, \n   \
      \     \"Loss\": loss\n    }\n\n    aiplatform.log_metrics(METRICS)\n\n    from\
      \ collections import namedtuple\n\n    component_outputs = namedtuple('ComponentOutputs',\
      \ \n                                   ['accuracy', 'loss', 'dep_decision'])\n\
      \n\n    return component_outputs(accuracy, loss, dep_decision)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - model_evaluation
