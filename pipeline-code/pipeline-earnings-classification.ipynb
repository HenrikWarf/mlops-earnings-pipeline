{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0430a772",
   "metadata": {},
   "source": [
    "# Productionazing Machine Learning with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5161d8-7450-474e-be6c-607e95ef2d88",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e1f209-c51a-4512-b31a-7251ca2a5424",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ud-bigquery) (1.23.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.3)\n",
      "Installing collected packages: google-cloud-bigquery\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.34.1\n",
      "    Uninstalling google-cloud-bigquery-2.34.1:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.34.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dbt-bigquery 1.1.0 requires google-cloud-bigquery<3,>=1.25.0, but you have google-cloud-bigquery 3.2.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-cloud-bigquery-3.2.0\n",
      "\n",
      "Usage:   \n",
      "  pip3 uninstall [options] <package> ...\n",
      "  pip3 uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: --user\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='libsecret', value='keyring.backends.libsecret', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/python-wheels/resolvelib-0.5.4-py2.py3-none-any.whl/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'google-cloud-pipeline-components'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/admin_/.local/lib/python3.9/site-packages/keyring/backend.py\", line 202, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/usr/lib/python3.9/importlib/metadata.py\", line 77, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'keyring.backends.libsecret'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='macOS', value='keyring.backends.macOS', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/python-wheels/resolvelib-0.5.4-py2.py3-none-any.whl/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'google-cloud-pipeline-components'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/admin_/.local/lib/python3.9/site-packages/keyring/backend.py\", line 202, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/usr/lib/python3.9/importlib/metadata.py\", line 77, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'keyring.backends.macOS'\u001b[0m\n",
      "Collecting google_cloud_pipeline_components\n",
      "  Downloading google_cloud_pipeline_components-1.0.13-py3-none-any.whl (600 kB)\n",
      "\u001b[K     |████████████████████████████████| 600 kB 11.4 MB/s \n",
      "\u001b[?25hCollecting google-cloud-notebooks>=0.4.0\n",
      "  Downloading google_cloud_notebooks-1.3.2-py2.py3-none-any.whl (354 kB)\n",
      "\u001b[K     |████████████████████████████████| 354 kB 25.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: kfp<2.0.0,>=1.8.9 in /home/admin_/.local/lib/python3.9/site-packages (from google_cloud_pipeline_components) (1.8.12)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/admin_/.local/lib/python3.9/site-packages (from google_cloud_pipeline_components) (1.31.6)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /home/admin_/.local/lib/python3.9/site-packages (from google_cloud_pipeline_components) (1.44.0)\n",
      "Collecting google-cloud-aiplatform<2,>=1.11.0\n",
      "  Downloading google_cloud_aiplatform-1.15.0-py2.py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 62.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz in /home/admin_/.local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (2022.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (1.56.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (1.16.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/admin_/.local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (20.9)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (3.19.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (2.28.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /home/admin_/.local/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (62.6.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/admin_/.local/lib/python3.9/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (4.8)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.5.1-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[K     |████████████████████████████████| 230 kB 48.8 MB/s \n",
      "\u001b[?25hCollecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Using cached google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (1.20.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (1.47.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (2.3.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /home/admin_/.local/lib/python3.9/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (1.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.9/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (0.12.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.9/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform<2,>=1.11.0->google_cloud_pipeline_components) (1.3.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.9.1)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.1.16)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.9/dist-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.9.1)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.4.0)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (5.4.1)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.1.1)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (2.1.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.14.1)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.2.13)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.0.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.12.11)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.8.2)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (18.20.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.1.10)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.8.10)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/admin_/.local/lib/python3.9/site-packages (from kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.4.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from Deprecated<2,>=1.2.7->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.14.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire<1,>=0.3.1->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.20.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.0.9)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (4.11.4)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (2022.6.15)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.9/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.26.9)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/admin_/.local/lib/python3.9/site-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.3.3)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.9/dist-packages (from kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/admin_/.local/lib/python3.9/site-packages (from pydantic<2,>=1.8.2->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google_cloud_pipeline_components) (2.0.12)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints<1,>=0.1.8->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (0.34.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp<2.0.0,>=1.8.9->google_cloud_pipeline_components) (3.2.0)\n",
      "Installing collected packages: google-cloud-resource-manager, google-cloud-bigquery, google-cloud-notebooks, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.2.0\n",
      "    Uninstalling google-cloud-bigquery-3.2.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.2.0\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/admin_/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed google-cloud-aiplatform-1.15.0 google-cloud-bigquery-2.34.4 google-cloud-notebooks-1.3.2 google-cloud-pipeline-components-1.0.13 google-cloud-resource-manager-1.5.1\n",
      "\n",
      "Usage:   \n",
      "  pip3 uninstall [options] <package> ...\n",
      "  pip3 uninstall [options] -r <requirements file> ...\n",
      "\n",
      "no such option: --user\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (8.0.0)\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='libsecret', value='keyring.backends.libsecret', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/python-wheels/resolvelib-0.5.4-py2.py3-none-any.whl/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'pyarrow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/admin_/.local/lib/python3.9/site-packages/keyring/backend.py\", line 202, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/usr/lib/python3.9/importlib/metadata.py\", line 77, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'keyring.backends.libsecret'\u001b[0m\n",
      "\u001b[31mERROR: Error initializing plugin EntryPoint(name='macOS', value='keyring.backends.macOS', group='keyring.backends').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/share/python-wheels/resolvelib-0.5.4-py2.py3-none-any.whl/resolvelib/resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'pyarrow'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/admin_/.local/lib/python3.9/site-packages/keyring/backend.py\", line 202, in _load_plugins\n",
      "    init_func = ep.load()\n",
      "  File \"/usr/lib/python3.9/importlib/metadata.py\", line 77, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'keyring.backends.macOS'\u001b[0m\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-8.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 13.6 MB/s \n",
      "\u001b[?25h  Downloading pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.7 MB 17.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user kfp --upgrade\n",
    "!pip3 uninstall --user google-cloud-bigquery -y\n",
    "!pip3 install --user google-cloud-bigquery --upgrade\n",
    "!pip3 uninstall --user google_cloud_pipeline_components -y\n",
    "!pip3 install --user google_cloud_pipeline_components --upgrade\n",
    "!pip3 uninstall --user pyarrow -y\n",
    "!pip3 install --user pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b5eea4-e596-4117-9b3e-a1e9140a8c18",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFP SDK version: 1.8.12\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30726c38-3a6b-4829-a872-206a678a0a87",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/admin_/.local/lib/python3.9/site-packages/google_cloud_bigquery-2.34.1.dist-info/METADATA'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:3037\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__dep_map\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2834\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 2834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr)\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_provider, attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:3028\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pkg_info\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2834\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 2834\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr)\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_provider, attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: _pkg_info",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m component\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIPlatformClient\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_cloud_pipeline_components\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform \u001b[38;5;28;01mas\u001b[39;00m gcc_aip\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Input,\n\u001b[1;32m     12\u001b[0m     Output,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     InputPath\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google_cloud_pipeline_components/aiplatform/__init__.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"Core modules for AI Platform Pipeline Components.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m aiplatform \u001b[38;5;28;01mas\u001b[39;00m aiplatform_sdk\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle_cloud_pipeline_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform/__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m aiplatform_version\n\u001b[1;32m     21\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m aiplatform_version\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializer\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     ImageDataset,\n\u001b[1;32m     28\u001b[0m     TabularDataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     VideoDataset,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m explain\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform/initializer.py:31\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m credentials \u001b[38;5;28;01mas\u001b[39;00m auth_credentials\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleAuthError\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base \u001b[38;5;28;01mas\u001b[39;00m constants\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform/compat/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m services\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m     21\u001b[0m V1BETA1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1beta1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform/compat/services/__init__.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 Google LLC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m dataset_service_client_v1beta1,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mendpoint_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m endpoint_service_client_v1beta1,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maiplatform_v1beta1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeaturestore_online_serving_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     client \u001b[38;5;28;01mas\u001b[39;00m featurestore_online_serving_service_client_v1beta1,\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform_v1beta1/__init__.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2022 Google LLC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceClient\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceAsyncClient\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mendpoint_service\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EndpointServiceClient\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform_v1beta1/services/dataset_service/__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2022 Google LLC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceClient\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceAsyncClient\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetServiceClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetServiceAsyncClient\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform_v1beta1/services/dataset_service/client.py:56\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timestamp_pb2  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceTransport, DEFAULT_CLIENT_INFO\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceGrpcTransport\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc_asyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceGrpcAsyncIOTransport\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform_v1beta1/services/dataset_service/transports/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Type\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceTransport\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceGrpcTransport\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc_asyncio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetServiceGrpcAsyncIOTransport\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/aiplatform_v1beta1/services/dataset_service/transports/base.py:41\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlongrunning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m operations_pb2  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     DEFAULT_CLIENT_INFO \u001b[38;5;241m=\u001b[39m gapic_v1\u001b[38;5;241m.\u001b[39mclient_info\u001b[38;5;241m.\u001b[39mClientInfo(\n\u001b[0;32m---> 41\u001b[0m         gapic_version\u001b[38;5;241m=\u001b[39m\u001b[43mpkg_resources\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-cloud-aiplatform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion,\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pkg_resources\u001b[38;5;241m.\u001b[39mDistributionNotFound:\n\u001b[1;32m     46\u001b[0m     DEFAULT_CLIENT_INFO \u001b[38;5;241m=\u001b[39m gapic_v1\u001b[38;5;241m.\u001b[39mclient_info\u001b[38;5;241m.\u001b[39mClientInfo()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:478\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    476\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Requirement\u001b[38;5;241m.\u001b[39mparse(dist)\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> 478\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[43mget_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected string, Requirement, or Distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:354\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m working_set\u001b[38;5;241m.\u001b[39mfind(moduleOrReq) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mrequire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmoduleOrReq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:909\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mrequirements):\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;124;03m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m     needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequirements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m needed:\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:803\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VersionConflict(dist, req)\u001b[38;5;241m.\u001b[39mwith_context(dependent_req)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# push the new requirements onto the stack\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m new_requirements \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextras\u001b[49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    804\u001b[0m requirements\u001b[38;5;241m.\u001b[39mextend(new_requirements)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Register the new requirements needed by req\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2755\u001b[0m, in \u001b[0;36mDistribution.requires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires\u001b[39m(\u001b[38;5;28mself\u001b[39m, extras\u001b[38;5;241m=\u001b[39m()):\n\u001b[1;32m   2754\u001b[0m     \u001b[38;5;124;03m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2755\u001b[0m     dm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dep_map\u001b[49m\n\u001b[1;32m   2756\u001b[0m     deps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2757\u001b[0m     deps\u001b[38;5;241m.\u001b[39mextend(dm\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28;01mNone\u001b[39;00m, ()))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:3039\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3039\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dep_map\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:3048\u001b[0m, in \u001b[0;36mDistInfoDistribution._compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3046\u001b[0m reqs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;66;03m# Including any condition expressions\u001b[39;00m\n\u001b[0;32m-> 3048\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m req \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parsed_pkg_info\u001b[49m\u001b[38;5;241m.\u001b[39mget_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequires-Dist\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m   3049\u001b[0m     reqs\u001b[38;5;241m.\u001b[39mextend(parse_requirements(req))\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreqs_for_extra\u001b[39m(extra):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:3030\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3030\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPKG_INFO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info \u001b[38;5;241m=\u001b[39m email\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mParser()\u001b[38;5;241m.\u001b[39mparsestr(metadata)\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pkg_info\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:1431\u001b[0m, in \u001b[0;36mNullProvider.get_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1430\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_path(name)\n\u001b[0;32m-> 1431\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:1635\u001b[0m, in \u001b[0;36mDefaultProvider._get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m-> 1635\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m   1636\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/admin_/.local/lib/python3.9/site-packages/google_cloud_bigquery-2.34.1.dist-info/METADATA'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Model,\n",
    "    Dataset,\n",
    "    Metrics,\n",
    "    InputPath\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcc7ad",
   "metadata": {},
   "source": [
    "### Set up project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea7e2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  mlops-dev-999-c6b8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"mlops-dev-999\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f600e4",
   "metadata": {},
   "source": [
    "### Define Current Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8247ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp:  20220629151703\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "print('Timestamp: ', TIMESTAMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537e110a",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae676e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://crazy-pipelines\"\n",
    "REGION = \"us-central1\" \n",
    "ML_PROJECT_NAME = \"earnings_classifier\"\n",
    "USER = \"crazy-hippo\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1e622",
   "metadata": {},
   "source": [
    "If you need to create the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d171802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://crazy-pipelines/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'crazy-pipelines' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb01f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://crazy-pipelines/earnings_classifier/\n",
      "                                 gs://crazy-pipelines/models/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21b64d",
   "metadata": {},
   "source": [
    "### Set Pipeline Root Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7d503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://crazy-pipelines/earnings_classifier/crazy-hippo'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "#Define Pipeline Root\n",
    "PIPELINE_ROOT = \"{}/{}/{}\".format(BUCKET_NAME, ML_PROJECT_NAME, USER)\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba025c",
   "metadata": {},
   "source": [
    "### Define Pipeline Comnponents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d2013",
   "metadata": {},
   "source": [
    "#### Pipeline Step 1 - Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa0a36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='data_ingestion.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs',\n",
    "                             'numpy',\n",
    "                             'kfp',\n",
    "                             'db_dtypes'\n",
    "                              ])\n",
    "def data_ingestion(\n",
    "        INPUT_DATA : str,\n",
    "        DATASET_VERSION : int,\n",
    "        DATASET : Output[Dataset],\n",
    "        pipeline_metrics: Output[Metrics]) -> NamedTuple(\n",
    "          'ComponentOutputs',\n",
    "          [\n",
    "              ('dataset_name', str),\n",
    "              ('dataset_version', int),\n",
    "              ('num_of_examples', int),\n",
    "              ('categorical_col', int),\n",
    "              ('numeric_col', int)\n",
    "          ]\n",
    "    ):\n",
    "    \n",
    "    #Import libraries\n",
    "    \n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import numpy\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    from google.cloud import bigquery\n",
    "    import random\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    \n",
    "    examples = random.randint(10000, 32561)\n",
    "    \n",
    "    client = Client(project='mlops-dev-999-c6b8', location=\"us\")\n",
    "    query = \"\"\"SELECT age, workclass, occupation, education_num, marital_status, capital_gain, label\n",
    "    FROM `mlops-dev-999-c6b8.earnings_prediction.earnings_raw_data` \n",
    "    LIMIT @examples\n",
    "    \"\"\"\n",
    "    \n",
    "    #Run Query\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"examples\", \"INT64\", examples),\n",
    "        ]\n",
    ")\n",
    "    job = client.query(query, job_config=job_config)\n",
    "    df = job.to_dataframe()\n",
    "    \n",
    "    #Set and calculate Dataset Metadata\n",
    "    \n",
    "    dataset_name = INPUT_DATA\n",
    "    dataset_version = DATASET_VERSION\n",
    "    num_of_examples = len(df)\n",
    "    \n",
    "    #Counting Data Types\n",
    "    \n",
    "    categorical_col = 0\n",
    "    numeric_col = 0\n",
    "    for col in df.columns : \n",
    "        print(type(df[col][0]))\n",
    "        if type(df[col][0]) == str :  \n",
    "            categorical_col += 1\n",
    "        elif type(df[col][0]) == numpy.int64 :\n",
    "            numeric_col += 1\n",
    "            \n",
    "    \n",
    "    #Write data to GCS \n",
    "    \n",
    "    df.to_csv(DATASET.path, index=False, header=True)\n",
    "    \n",
    "    # Log Metrics\n",
    "    \n",
    "    pipeline_metrics.log_metric('dataset_name', dataset_name)\n",
    "    pipeline_metrics.log_metric('dataset_version', dataset_version)\n",
    "    pipeline_metrics.log_metric('num_of_examples', num_of_examples)\n",
    "    pipeline_metrics.log_metric('categorical_col', categorical_col)\n",
    "    pipeline_metrics.log_metric('numeric_col', numeric_col)\n",
    "    \n",
    "    \n",
    "    #Outputs of Component defined by named tuple\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    component_outputs = namedtuple('ComponentOutputs',\n",
    "        ['dataset_name', \n",
    "         'dataset_version', \n",
    "         'num_of_examples', \n",
    "         'categorical_col', \n",
    "         'numeric_col'])\n",
    "        \n",
    "    #Returning outputs\n",
    "    \n",
    "    return component_outputs(dataset_name, \n",
    "                             dataset_version, \n",
    "                             num_of_examples, \n",
    "                             categorical_col, \n",
    "                             numeric_col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc50bd",
   "metadata": {},
   "source": [
    "#### Pipeline Step 2 - Transform and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f267325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='data_transformation.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'google-cloud-bigquery', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs', \n",
    "                             'sklearn',\n",
    "                             'db_dtypes'])\n",
    "def data_transformation(\n",
    "        DATASET : Input[Dataset],\n",
    "        TRAINING_DATA : Output[Dataset],\n",
    "        TEST_DATA : Output[Dataset],\n",
    "        VALIDATION_DATA : Output[Dataset]\n",
    "    ):\n",
    "    \n",
    "    #Import libraries\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    df = pd.read_csv(DATASET.path)\n",
    "    \n",
    "    #Drop null values in dataset\n",
    "    df = df.dropna()\n",
    "    \n",
    "    #Transform label to integer data type and format 1 or 0\n",
    "    df['label'] = [int(1) if x == '>50K' else int(0) for x in df['label']]\n",
    "    \n",
    "    #Create training, test and validation datasets\n",
    "    train, test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "    train, val = train_test_split(train, test_size=0.20, random_state=42)\n",
    "    \n",
    "    print(TRAINING_DATA.path)\n",
    "    print(TEST_DATA.path)\n",
    "    print(VALIDATION_DATA.path)\n",
    "\n",
    "    #Write data to GCS Storage\n",
    "    train.to_csv(TRAINING_DATA.path, index=False, header=True)\n",
    "    test.to_csv(TEST_DATA.path, index=False, header=True)\n",
    "    val.to_csv(VALIDATION_DATA.path, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63e5f0",
   "metadata": {},
   "source": [
    "#### Pipeline Step 3 - Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc418fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='model_training.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas', \n",
    "                             'pyarrow' , \n",
    "                             'gcsfs' , \n",
    "                             'google-cloud-bigquery-storage',\n",
    "                             'tensorflow==2.5',\n",
    "                             'db_dtypes'])\n",
    "def model_training(TRAINING_DATA: Input[Dataset], \n",
    "                TEST_DATA: Input[Dataset], \n",
    "                VALIDATION_DATA: Input[Dataset],\n",
    "                MODEL: Output[Model]\n",
    "               ):\n",
    "    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras.layers.experimental import preprocessing\n",
    "    \n",
    "    #VARIABLES AND TRAINING PARAMETERS\n",
    "    TRAIN_DATA = pd.read_csv(TRAINING_DATA.path)\n",
    "    TEST_DATA = pd.read_csv(TEST_DATA.path)\n",
    "    VAL_DATA = pd.read_csv(VALIDATION_DATA.path)\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    print(tf.__version__)\n",
    "    \n",
    "    print(MODEL.path)\n",
    "\n",
    "    #TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('label')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #NORMALIZATION FUNCTION\n",
    "    def helperfunc_get_normalization_layer(name, dataset):\n",
    "        # Create a Normalization layer for our feature.\n",
    "        normalizer = preprocessing.Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the statistics of the data.\n",
    "        normalizer.adapt(feature_ds)\n",
    "\n",
    "        return normalizer\n",
    "    \n",
    "    #CATEGORY ENCODING FUNCTION\n",
    "    def helperfunc_get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "        # Create a StringLookup layer which will turn strings into integer indices\n",
    "        if dtype == 'string':\n",
    "            index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "        else:\n",
    "            index = preprocessing.IntegerLookup(max_values=max_tokens)\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "        # Learn the set of possible values and assign them a fixed integer index.\n",
    "        index.adapt(feature_ds)\n",
    "\n",
    "        # Create a Discretization for our integer indices.\n",
    "        encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature.\n",
    "        feature_ds = feature_ds.map(index)\n",
    "\n",
    "        # Learn the space of possible indices.\n",
    "        encoder.adapt(feature_ds)\n",
    "\n",
    "        # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "        # layer so we can use them, or include them in the functional model later.\n",
    "        return lambda feature: encoder(index(feature))\n",
    "    \n",
    "    #CREATE TENSORFLOW DATASETS\n",
    "    TRAIN_DS = helperfunc_create_dataset(TRAIN_DATA, batch_size=BATCH_SIZE)\n",
    "    VALIDATION_DS = helperfunc_create_dataset(VAL_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    TESTING_DS = helperfunc_create_dataset(TEST_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #CREATE PREPROCESSING LAYERS\n",
    "    ALL_INPUTS = []\n",
    "    ENCODED_FEATURES = []\n",
    "\n",
    "    NUMERICAL = ['age' , 'capital_gain']\n",
    "    CATEGORICAL_INT_COLS = ['education_num']\n",
    "    CATEGORICAL_STRING_COLS = ['occupation', \n",
    "                               'workclass', \n",
    "                               'marital_status']\n",
    "    TARGET = ['label']\n",
    "    \n",
    "    # Numeric features.\n",
    "    for header in NUMERICAL:\n",
    "        numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "        normalization_layer = helperfunc_get_normalization_layer(header, TRAIN_DS)\n",
    "        encoded_numeric_col = normalization_layer(numeric_col)\n",
    "        ALL_INPUTS.append(numeric_col)\n",
    "        ENCODED_FEATURES.append(encoded_numeric_col)\n",
    "        \n",
    "    # Categorical features encoded as integers.\n",
    "    for header in CATEGORICAL_INT_COLS:\n",
    "        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='int64', max_tokens=5)\n",
    "        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n",
    "        ALL_INPUTS.append(categorical_int_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_int_col)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "    for header in CATEGORICAL_STRING_COLS:\n",
    "        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='string', max_tokens=5)\n",
    "        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n",
    "        ALL_INPUTS.append(categorical_string_col)\n",
    "        ENCODED_FEATURES.append(encoded_categorical_string_col)\n",
    "    \n",
    "        \n",
    "    #CREATE and COMPILE MODEL\n",
    "    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n",
    "    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(ALL_INPUTS, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    #TRAIN MODEL\n",
    "    history = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n",
    "    \n",
    "    \n",
    "    #Define Bucket in GCS for Model Storage\n",
    "    BUCKET = 'gs://crazy-pipelines/models/'\n",
    "    \n",
    "    #Define MODEL PATH \n",
    "    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n",
    "    \n",
    "    MODEL.uri = MODEL_PATH \n",
    "    \n",
    "    \n",
    "    #Save model to Artifact Store for Project\n",
    "    model.save(MODEL.path)\n",
    "    \n",
    "    print('Model saved to: ' + MODEL.path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bcbd9",
   "metadata": {},
   "source": [
    "#### Pipeline Step 4 - Evaluate Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9ddb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='model_evaluation.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas',\n",
    "                         'google-cloud-bigquery',\n",
    "                         'pyarrow', \n",
    "                         'gcsfs',\n",
    "                         'tensorflow==2.5',\n",
    "                         'google-cloud-aiplatform',\n",
    "                         'db_dtypes'])\n",
    "def model_evaluation(MODEL : Input[Model], \n",
    "                            TEST_DATA: Input[Dataset], \n",
    "                            num_of_examples: int,\n",
    "                            categorical_col: int,\n",
    "                            numeric_col: int,\n",
    "                            pipeline:str, \n",
    "                            framework:str,\n",
    "                            input_path:str,\n",
    "                            dataset_version:int,\n",
    "                            pipeline_metrics: Output[Metrics]) ->  NamedTuple(\n",
    "                                                                    'ComponentOutputs',\n",
    "                                                                              [\n",
    "                                                                                  ('accuracy', float),\n",
    "                                                                                  ('loss', float),\n",
    "                                                                                  ('dep_decision',str)\n",
    "                                                                              ]):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    \n",
    "    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n",
    "    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n",
    "        dataframe = dataframe.copy()\n",
    "        labels = dataframe.pop('label')\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.prefetch(batch_size)\n",
    "        return ds\n",
    "    \n",
    "    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n",
    "    reloaded_model = tf.keras.models.load_model(MODEL.path)\n",
    "    \n",
    "    #READ TESTING DATASET\n",
    "    TESTING_DATA = pd.read_csv(TEST_DATA.path)\n",
    "\n",
    "    #SET BATCG SIZE\n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n",
    "    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    #EVALUATE MODEL WITH TEST DATA\n",
    "    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n",
    "    \n",
    "    accuracy = float(accuracy)\n",
    "    loss = float(loss)\n",
    "    dep_decision = 'false'\n",
    "    \n",
    "    #PRINT ACCURACY METRIC\n",
    "    print(\"Accuracy\", accuracy)\n",
    "    print(\"Loss\", loss)\n",
    "    \n",
    "    \n",
    "    from tensorflow.python.lib.io import file_io    \n",
    "    \n",
    "    #Write Metrics to BigQuery Table for Validation and possible promotion to Deployment\n",
    "    from google.cloud.bigquery import Client, QueryJobConfig\n",
    "    \n",
    "    #Initiate BigQuery Client\n",
    "    client = Client(project='mlops-dev-999-c6b8', location=\"us\")\n",
    "    \n",
    "    print('Sending Metrics into BigQuery')\n",
    "    \n",
    "    #Define DML Query to Insert Metrics into BugQuery\n",
    "    query = \"\"\"INSERT `mlops-dev-999-c6b8.earnings_prediction.model_metrics_history` (model_name, pipeline, framework, accuracy, loss)\n",
    "    VALUES (\"{}\", \"{}\", \"{}\", {}, {})  \n",
    "    \"\"\".format(MODEL.path, pipeline, framework, accuracy, loss)\n",
    "    \n",
    "    #Run Query\n",
    "    job = client.query(query)\n",
    "    \n",
    "    print('Metrics sent to BigQuery!')\n",
    "    \n",
    "    # Export two metrics\n",
    "    pipeline_metrics.log_metric('accuracy', accuracy)\n",
    "    pipeline_metrics.log_metric('loss', loss)\n",
    "\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    component_outputs = namedtuple('ComponentOutputs',\n",
    "        ['accuracy', 'loss'])\n",
    "    \n",
    "    \n",
    "    from datetime import datetime\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    MY_PROJECT = 'mlops-dev-999-c6b8'\n",
    "    REGION = 'us-central1'\n",
    "    EXPERIMENT_NAME = 'earnings-classifier-ver1'\n",
    "    RUN_NAME = \"tensorflow-dl-model-\" + TIMESTAMP\n",
    "    \n",
    "    \n",
    "    #Store Experiment Metrics in Vertex AI\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=MY_PROJECT, location=REGION, experiment=EXPERIMENT_NAME)\n",
    "    aiplatform.start_run(run=RUN_NAME)\n",
    "    \n",
    "    PARAMETERS = {\n",
    "        #\"Model\" : MODEL,\n",
    "        #\"Pipeline\" : pipeline,\n",
    "        #\"Dataset\" : input_path,\n",
    "        \"Dataset Version\" : dataset_version\n",
    "    }\n",
    "    \n",
    "    aiplatform.log_params(PARAMETERS)\n",
    "    \n",
    "    METRICS = {\n",
    "        'Num_of_examples': num_of_examples,\n",
    "        'Categorical_col': categorical_col,\n",
    "        'Numeric_col': numeric_col,\n",
    "        \"Accuracy\": accuracy, \n",
    "        \"Loss\": loss\n",
    "    }\n",
    "    \n",
    "    aiplatform.log_metrics(METRICS)\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    \n",
    "    component_outputs = namedtuple('ComponentOutputs', \n",
    "                                   ['accuracy', 'loss', 'dep_decision'])\n",
    "    \n",
    "        \n",
    "    return component_outputs(accuracy, loss, dep_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d9b1ad0-f013-4994-b1dd-e09f679307ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(output_component_file='model_deploy.yaml',\n",
    "          base_image='python:3.9',\n",
    "          packages_to_install=['pandas',\n",
    "                         'google-cloud-bigquery',\n",
    "                         'pyarrow', \n",
    "                         'gcsfs',\n",
    "                         'tensorflow==2.5',\n",
    "                         'google-cloud-aiplatform',\n",
    "                         'db_dtypes'])\n",
    "def model_deploy():\n",
    "    \n",
    "    \n",
    "    print(\"Model Deployed to Endpoint\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb65bb",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e108d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='earnings-classifier',\n",
    "  description='Binary Classification Model with Tensorflow Deep Learning and Connected Pre-processing Layers'\n",
    ")\n",
    "def binary_classifier_earnings_v2(\n",
    "    pipeline: str = 'DL Version 2 (Tensorflow)',\n",
    "    framework: str = 'Tensorflow',\n",
    "    input_path: str = 'mlops-dev-999-c6b8.earnings_prediction.earnings_raw_data',\n",
    "    dataset_version: int = 2\n",
    "    ):\n",
    "    \n",
    "    first_step = data_ingestion(input_path,\n",
    "                              dataset_version)\n",
    "   \n",
    "    second_step = data_transformation(first_step.outputs['DATASET'])\n",
    "\n",
    "   \n",
    "    third_step = model_training(second_step.outputs['TRAINING_DATA'], \n",
    "                             second_step.outputs['TEST_DATA'], \n",
    "                             second_step.outputs['VALIDATION_DATA'])\n",
    "    \n",
    "\n",
    "    fourth_step = model_evaluation(third_step.outputs['MODEL'], \n",
    "                                          second_step.outputs['TEST_DATA'],\n",
    "                                          first_step.outputs['num_of_examples'],\n",
    "                                          first_step.outputs['categorical_col'],\n",
    "                                          first_step.outputs['numeric_col'],\n",
    "                                          pipeline, \n",
    "                                          framework, \n",
    "                                          input_path, \n",
    "                                          dataset_version,\n",
    "                                  )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        fourth_step.outputs[\"dep_decision\"] == \"true\",\n",
    "        name=\"deploy_decision\",\n",
    "     ):\n",
    "        \n",
    "        fift_step = model_deploy()\n",
    "        \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b0de5-2037-4d39-8f58-bfa6a67a8d11",
   "metadata": {},
   "source": [
    "Compile through KFP compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8cfe529-de5f-4e7f-a0fa-4eddaf878bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler  \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=binary_classifier_earnings_v2, package_path=\"earnings_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e8724-3b1b-4fca-ac18-98a5900622c4",
   "metadata": {},
   "source": [
    "### Test Pipeline in Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ebcf2-15e9-4fa0-9e10-c1f845657ddd",
   "metadata": {},
   "source": [
    "Set run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1fce66-372c-4390-a9b0-50509ea51bbf",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'PIPELINE_ROOT' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m COMPILED_PIPELINE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearnings_pipeline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m JOB_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m PIPELINE_ROOT_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mPIPELINE_ROOT\u001b[49m\n\u001b[1;32m      5\u001b[0m PIPELINE_PARAMETERS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m ENABLE_CACHING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PIPELINE_ROOT' is not defined"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"earnings-classifier-pipeline\"\n",
    "COMPILED_PIPELINE_PATH = \"earnings_pipeline.json\"\n",
    "JOB_ID = \"\"\n",
    "PIPELINE_ROOT_PATH = PIPELINE_ROOT\n",
    "PIPELINE_PARAMETERS = {}\n",
    "ENABLE_CACHING = False\n",
    "SERVICE_ACCOUNT = 'ml1-dev-sa@mlops-dev-999-c6b8.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785d2f2-f77b-4327-85ae-76cac08e43ae",
   "metadata": {},
   "source": [
    "Run pipeline in Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a2e0ffb-6c8d-4439-b082-a6399289afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/965234628650/locations/us-central1/pipelineJobs/earnings-classifier-20220629151728\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/965234628650/locations/us-central1/pipelineJobs/earnings-classifier-20220629151728')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/earnings-classifier-20220629151728?project=965234628650\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name = DISPLAY_NAME,\n",
    "                             template_path = COMPILED_PIPELINE_PATH,\n",
    "                             job_id = JOB_ID,\n",
    "                             pipeline_root = PIPELINE_ROOT_PATH,\n",
    "                             parameter_values = PIPELINE_PARAMETERS,\n",
    "                             enable_caching = ENABLE_CACHING,\n",
    "                             project = PROJECT_ID,\n",
    "                             location = REGION)\n",
    "\n",
    "job.submit(\n",
    "    service_account=SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6291d-2beb-40e8-8bcf-c08a0e629cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}