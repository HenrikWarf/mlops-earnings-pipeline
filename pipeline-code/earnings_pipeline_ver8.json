{
  "pipelineSpec": {
    "components": {
      "comp-data-ingestion": {
        "executorLabel": "exec-data-ingestion",
        "inputDefinitions": {
          "parameters": {
            "DATASET_VERSION": {
              "type": "INT"
            },
            "INPUT_DATA": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "DATASET": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "pipeline_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "categorical_col": {
              "type": "INT"
            },
            "dataset_name": {
              "type": "STRING"
            },
            "dataset_version": {
              "type": "INT"
            },
            "num_of_examples": {
              "type": "INT"
            },
            "numeric_col": {
              "type": "INT"
            }
          }
        }
      },
      "comp-data-transformation": {
        "executorLabel": "exec-data-transformation",
        "inputDefinitions": {
          "artifacts": {
            "DATASET": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "TEST_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "TRAINING_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "VALIDATION_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-model-evaluation": {
        "executorLabel": "exec-model-evaluation",
        "inputDefinitions": {
          "artifacts": {
            "MODEL": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            },
            "TEST_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "categorical_col": {
              "type": "INT"
            },
            "dataset_version": {
              "type": "INT"
            },
            "framework": {
              "type": "STRING"
            },
            "input_path": {
              "type": "STRING"
            },
            "num_of_examples": {
              "type": "INT"
            },
            "numeric_col": {
              "type": "INT"
            },
            "pipeline": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "pipeline_metrics": {
              "artifactType": {
                "schemaTitle": "system.Metrics",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "accuracy": {
              "type": "DOUBLE"
            },
            "loss": {
              "type": "DOUBLE"
            }
          }
        }
      },
      "comp-model-training": {
        "executorLabel": "exec-model-training",
        "inputDefinitions": {
          "artifacts": {
            "TEST_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "TRAINING_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "VALIDATION_DATA": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "MODEL": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-data-ingestion": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "data_ingestion"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'google-cloud-bigquery' 'pyarrow' 'gcsfs' 'numpy' 'kfp' 'db_dtypes' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef data_ingestion(\n        INPUT_DATA : str,\n        DATASET_VERSION : int,\n        DATASET : Output[Dataset],\n        pipeline_metrics: Output[Metrics]) -> NamedTuple(\n          'ComponentOutputs',\n          [\n              ('dataset_name', str),\n              ('dataset_version', int),\n              ('num_of_examples', int),\n              ('categorical_col', int),\n              ('numeric_col', int)\n          ]\n    ):\n\n    #Import libraries\n\n    import pandas as pd\n    import time\n    import numpy\n    from google.cloud.bigquery import Client, QueryJobConfig\n\n\n    #Initiate BigQuery Client\n\n    client = Client(project='crazy-hippo-01')\n    query = \"\"\"SELECT age, workclass, occupation, education_num, marital_status, capital_gain, income_bracket\n    FROM `crazy-hippo-01.census_data_us.census_raw` \n    LIMIT 20000\n    \"\"\"\n\n    #Run Query\n\n    job = client.query(query)\n    df = job.to_dataframe()\n\n    #Set and calculate Dataset Metadata\n\n    dataset_name = INPUT_DATA\n    dataset_version = DATASET_VERSION\n    num_of_examples = len(df)\n\n    #Counting Data Types\n\n    categorical_col = 0\n    numeric_col = 0\n    for col in df.columns : \n        print(type(df[col][0]))\n        if type(df[col][0]) == str :  \n            categorical_col += 1\n        elif type(df[col][0]) == numpy.int64 :\n            numeric_col += 1\n\n    #Write data to GCS \n\n    df.to_csv(DATASET.path, index=False, header=True)\n\n    # Log Metrics\n\n    pipeline_metrics.log_metric('dataset_name', dataset_name)\n    pipeline_metrics.log_metric('dataset_version', dataset_version)\n    pipeline_metrics.log_metric('num_of_examples', num_of_examples)\n    pipeline_metrics.log_metric('categorical_col', categorical_col)\n    pipeline_metrics.log_metric('numeric_col', numeric_col)\n\n\n    #Outputs of Component defined by named tuple\n\n    from collections import namedtuple\n    component_outputs = namedtuple('ComponentOutputs',\n        ['dataset_name', \n         'dataset_version', \n         'num_of_examples', \n         'categorical_col', \n         'numeric_col'])\n\n    #Returning outputs\n\n    return component_outputs(dataset_name, \n                             dataset_version, \n                             num_of_examples, \n                             categorical_col, \n                             numeric_col)\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-data-transformation": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "data_transformation"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'google-cloud-bigquery' 'pyarrow' 'gcsfs' 'sklearn' 'db_dtypes' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef data_transformation(\n        DATASET : Input[Dataset],\n        TRAINING_DATA : Output[Dataset],\n        TEST_DATA : Output[Dataset],\n        VALIDATION_DATA : Output[Dataset]\n    ):\n\n    #Import libraries\n    import pandas as pd\n    import time\n    from sklearn.model_selection import train_test_split\n    from google.cloud.bigquery import Client, QueryJobConfig\n\n    df = pd.read_csv(DATASET.path)\n\n    #Drop null values in dataset\n    df = df.dropna()\n\n    #Create training, test and validation datasets\n    train, test = train_test_split(df, test_size=0.20, random_state=42)\n    train, val = train_test_split(train, test_size=0.20, random_state=42)\n\n    #Define Staging Bucket in GCS\n    #BUCKET = 'gcs://crazy-hippo-01/kubeflow_staging/earnings_model/'\n    #TRAINING_PATH = BUCKET + 'datasets/training/training{}'.format(str(int(time.time())))  + '.csv'\n    #TEST_PATH = BUCKET + 'datasets/testing/test{}'.format(str(int(time.time())))  + '.csv'\n    #VALIDATION_PATH = BUCKET + 'datasets/validation/validation{}'.format(str(int(time.time())))  + '.csv'\n\n    #Define Datasets Names\n    #TRAINING_DATA.uri = TRAINING_PATH\n    #TEST_DATA.uri = TEST_PATH\n    #VALIDATION_DATA.uri = VALIDATION_PATH\n\n    print(TRAINING_DATA.path)\n    print(TEST_DATA.path)\n    print(VALIDATION_DATA.path)\n\n    #Write data to GCS Storage\n    train.to_csv(TRAINING_DATA.path, index=False, header=True)\n    test.to_csv(TEST_DATA.path, index=False, header=True)\n    val.to_csv(VALIDATION_DATA.path, index=False, header=True)\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-model-evaluation": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "model_evaluation"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'google-cloud-bigquery' 'pyarrow' 'gcsfs' 'tensorflow==2.5' 'google-cloud-aiplatform' 'db_dtypes' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef model_evaluation(\n                            MODEL : Input[Model], \n                            TEST_DATA: Input[Dataset], \n                            num_of_examples: int,\n                            categorical_col: int,\n                            numeric_col: int,\n                            pipeline:str, \n                            framework:str,\n                            input_path:str,\n                            dataset_version:int,\n                            pipeline_metrics: Output[Metrics]) -> NamedTuple(\n      'ComponentOutputs',\n      [\n          ('accuracy', float),\n          ('loss', float),\n      ]):\n\n    import pandas as pd\n    import tensorflow as tf\n    from tensorflow import keras\n\n    #HELPER FUNCTION - TENSORFLOW DATASET FUNCTION\n    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n        dataframe = dataframe.copy()\n        labels = dataframe.pop('income_bracket')\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n        if shuffle:\n            ds = ds.shuffle(buffer_size=len(dataframe))\n        ds = ds.batch(batch_size)\n        ds = ds.prefetch(batch_size)\n        return ds\n\n    #LOAD TRAINED MODEL FROM ARTIFACT STORE\n    reloaded_model = tf.keras.models.load_model(MODEL.path)\n\n    #READ TESTING DATASET\n    TESTING_DATA = pd.read_csv(TEST_DATA.path)\n\n    #SET BATCG SIZE\n    BATCH_SIZE = 32\n\n    #CALL HELPER FUNCTION TO CREATE TENSORFLOW DATASET\n    TESTING_DS = helperfunc_create_dataset(TESTING_DATA, shuffle=False, batch_size=BATCH_SIZE)\n\n    #EVALUATE MODEL WITH TEST DATA\n    loss, accuracy = reloaded_model.evaluate(TESTING_DS)\n\n    accuracy = float(accuracy)\n    loss = float(loss)\n\n    #PRINT ACCURACY METRIC\n    print(\"Accuracy\", accuracy)\n    print(\"Loss\", loss)\n\n\n    from tensorflow.python.lib.io import file_io    \n\n    #Write Metrics to BigQuery Table for Validation and possible promotion to Deployment\n    from google.cloud.bigquery import Client, QueryJobConfig\n\n    #Initiate BigQuery Client\n    client = Client(project='crazy-hippo-01')\n\n    print('Sending Metrics into BigQuery')\n\n    #Define DML Query to Insert Metrics into BugQuery\n    query = \"\"\"INSERT `crazy-hippo-01.census_data_us.model_metrics_history` (model_name, pipeline, framework, accuracy, loss)\n    VALUES (\"{}\", \"{}\", \"{}\", {}, {})  \n    \"\"\".format(MODEL.path, pipeline, framework, accuracy, loss)\n\n    #Run Query\n    job = client.query(query)\n\n    print('Metrics sent to BigQuery!')\n\n    # Export two metrics\n    pipeline_metrics.log_metric('accuracy', accuracy)\n    pipeline_metrics.log_metric('loss', loss)\n\n    from collections import namedtuple\n\n    component_outputs = namedtuple('ComponentOutputs',\n        ['accuracy', 'loss'])\n\n\n    from datetime import datetime\n\n    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    MY_PROJECT = 'crazy-hippo-01'\n    REGION = 'us-central1'\n    EXPERIMENT_NAME = 'earnings-classifier-ver1'\n    RUN_NAME = \"tensorflow-dl-model-\" + TIMESTAMP\n\n\n    #Store Experiment Metrics in Vertex AI\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=MY_PROJECT, location=REGION, experiment=EXPERIMENT_NAME)\n    aiplatform.start_run(run=RUN_NAME)\n\n    PARAMETERS = {\n        #\"Model\" : MODEL,\n        #\"Pipeline\" : pipeline,\n        #\"Dataset\" : input_path,\n        \"Dataset Version\" : dataset_version\n    }\n\n    aiplatform.log_params(PARAMETERS)\n\n    METRICS = {\n        'Num_of_examples': num_of_examples,\n        'Categorical_col': categorical_col,\n        'Numeric_col': numeric_col,\n        \"Accuracy\": accuracy, \n        \"Loss\": loss\n    }\n\n    aiplatform.log_metrics(METRICS)\n\n    from collections import namedtuple\n\n    component_outputs = namedtuple('ComponentOutputs', \n                                   ['accuracy', 'loss'])\n\n\n    return component_outputs(accuracy, loss)\n\n"
            ],
            "image": "python:3.9"
          }
        },
        "exec-model-training": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "model_training"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'gcsfs' 'google-cloud-bigquery-storage' 'tensorflow==2.5' 'db_dtypes' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef model_training(TRAINING_DATA: Input[Dataset], \n                TEST_DATA: Input[Dataset], \n                VALIDATION_DATA: Input[Dataset],\n                MODEL: Output[Model]\n               ):\n\n\n    import pandas as pd\n    import time\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers\n    from tensorflow.keras.layers.experimental import preprocessing\n\n    #VARIABLES AND TRAINING PARAMETERS\n    TRAIN_DATA = pd.read_csv(TRAINING_DATA.path)\n    TEST_DATA = pd.read_csv(TEST_DATA.path)\n    VAL_DATA = pd.read_csv(VALIDATION_DATA.path)\n\n    BATCH_SIZE = 32\n\n    print(tf.__version__)\n\n    print(MODEL.path)\n\n    #TENSORFLOW DATASET FUNCTION\n    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n        dataframe = dataframe.copy()\n        labels = dataframe.pop('income_bracket')\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n        if shuffle:\n            ds = ds.shuffle(buffer_size=len(dataframe))\n        ds = ds.batch(batch_size)\n        ds = ds.prefetch(batch_size)\n        return ds\n\n    #NORMALIZATION FUNCTION\n    def helperfunc_get_normalization_layer(name, dataset):\n        # Create a Normalization layer for our feature.\n        normalizer = preprocessing.Normalization()\n\n        # Prepare a Dataset that only yields our feature.\n        feature_ds = dataset.map(lambda x, y: x[name])\n\n        # Learn the statistics of the data.\n        normalizer.adapt(feature_ds)\n\n        return normalizer\n\n    #CATEGORY ENCODING FUNCTION\n    def helperfunc_get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n        # Create a StringLookup layer which will turn strings into integer indices\n        if dtype == 'string':\n            index = preprocessing.StringLookup(max_tokens=max_tokens)\n        else:\n            index = preprocessing.IntegerLookup(max_values=max_tokens)\n\n        # Prepare a Dataset that only yields our feature\n        feature_ds = dataset.map(lambda x, y: x[name])\n\n        # Learn the set of possible values and assign them a fixed integer index.\n        index.adapt(feature_ds)\n\n        # Create a Discretization for our integer indices.\n        encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n\n        # Prepare a Dataset that only yields our feature.\n        feature_ds = feature_ds.map(index)\n\n        # Learn the space of possible indices.\n        encoder.adapt(feature_ds)\n\n        # Apply one-hot encoding to our indices. The lambda function captures the\n        # layer so we can use them, or include them in the functional model later.\n        return lambda feature: encoder(index(feature))\n\n    #CREATE TENSORFLOW DATASETS\n    TRAIN_DS = helperfunc_create_dataset(TRAIN_DATA, batch_size=BATCH_SIZE)\n    VALIDATION_DS = helperfunc_create_dataset(VAL_DATA, shuffle=False, batch_size=BATCH_SIZE)\n    TESTING_DS = helperfunc_create_dataset(TEST_DATA, shuffle=False, batch_size=BATCH_SIZE)\n\n    #CREATE PREPROCESSING LAYERS\n    ALL_INPUTS = []\n    ENCODED_FEATURES = []\n\n    NUMERICAL = ['age' , 'capital_gain']\n    CATEGORICAL_INT_COLS = ['education_num']\n    CATEGORICAL_STRING_COLS = ['occupation', \n                               'workclass', \n                               'marital_status']\n    TARGET = ['income_bracket']\n\n    # Numeric features.\n    for header in NUMERICAL:\n        numeric_col = tf.keras.Input(shape=(1,), name=header)\n        normalization_layer = helperfunc_get_normalization_layer(header, TRAIN_DS)\n        encoded_numeric_col = normalization_layer(numeric_col)\n        ALL_INPUTS.append(numeric_col)\n        ENCODED_FEATURES.append(encoded_numeric_col)\n\n    # Categorical features encoded as integers.\n    for header in CATEGORICAL_INT_COLS:\n        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='int64', max_tokens=5)\n        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n        ALL_INPUTS.append(categorical_int_col)\n        ENCODED_FEATURES.append(encoded_categorical_int_col)\n\n    # Categorical features encoded as string.\n    for header in CATEGORICAL_STRING_COLS:\n        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS, dtype='string', max_tokens=5)\n        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n        ALL_INPUTS.append(categorical_string_col)\n        ENCODED_FEATURES.append(encoded_categorical_string_col)\n\n\n    #CREATE and COMPILE MODEL\n    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(ALL_INPUTS, output)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=[\"accuracy\"])\n\n    #TRAIN MODEL\n    history = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n\n\n    #Define Bucket in GCS for Model Storage\n    BUCKET = 'gs://crazy-hippo-01/kubeflow_staging/earnings_model/models/'\n\n    #Define MODEL PATH \n    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n\n    MODEL.uri = MODEL_PATH \n\n\n    #Save model to Artifact Store for Project\n    model.save(MODEL.path)\n\n    print('Model saved to: ' + MODEL.path)\n\n"
            ],
            "image": "python:3.9"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "binaryclassmodel8"
    },
    "root": {
      "dag": {
        "outputs": {
          "artifacts": {
            "data-ingestion-pipeline_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "pipeline_metrics",
                  "producerSubtask": "data-ingestion"
                }
              ]
            },
            "model-evaluation-pipeline_metrics": {
              "artifactSelectors": [
                {
                  "outputArtifactKey": "pipeline_metrics",
                  "producerSubtask": "model-evaluation"
                }
              ]
            }
          }
        },
        "tasks": {
          "data-ingestion": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-data-ingestion"
            },
            "inputs": {
              "parameters": {
                "DATASET_VERSION": {
                  "componentInputParameter": "dataset_version"
                },
                "INPUT_DATA": {
                  "componentInputParameter": "input_path"
                }
              }
            },
            "taskInfo": {
              "name": "data-ingestion"
            }
          },
          "data-transformation": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-data-transformation"
            },
            "dependentTasks": [
              "data-ingestion"
            ],
            "inputs": {
              "artifacts": {
                "DATASET": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "DATASET",
                    "producerTask": "data-ingestion"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "data-transformation"
            }
          },
          "model-evaluation": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-model-evaluation"
            },
            "dependentTasks": [
              "data-ingestion",
              "data-transformation",
              "model-training"
            ],
            "inputs": {
              "artifacts": {
                "MODEL": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "MODEL",
                    "producerTask": "model-training"
                  }
                },
                "TEST_DATA": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "TEST_DATA",
                    "producerTask": "data-transformation"
                  }
                }
              },
              "parameters": {
                "categorical_col": {
                  "taskOutputParameter": {
                    "outputParameterKey": "categorical_col",
                    "producerTask": "data-ingestion"
                  }
                },
                "dataset_version": {
                  "componentInputParameter": "dataset_version"
                },
                "framework": {
                  "componentInputParameter": "framework"
                },
                "input_path": {
                  "componentInputParameter": "input_path"
                },
                "num_of_examples": {
                  "taskOutputParameter": {
                    "outputParameterKey": "num_of_examples",
                    "producerTask": "data-ingestion"
                  }
                },
                "numeric_col": {
                  "taskOutputParameter": {
                    "outputParameterKey": "numeric_col",
                    "producerTask": "data-ingestion"
                  }
                },
                "pipeline": {
                  "componentInputParameter": "pipeline"
                }
              }
            },
            "taskInfo": {
              "name": "model-evaluation"
            }
          },
          "model-training": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-model-training"
            },
            "dependentTasks": [
              "data-transformation"
            ],
            "inputs": {
              "artifacts": {
                "TEST_DATA": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "TEST_DATA",
                    "producerTask": "data-transformation"
                  }
                },
                "TRAINING_DATA": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "TRAINING_DATA",
                    "producerTask": "data-transformation"
                  }
                },
                "VALIDATION_DATA": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "VALIDATION_DATA",
                    "producerTask": "data-transformation"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "model-training"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "dataset_version": {
            "type": "INT"
          },
          "framework": {
            "type": "STRING"
          },
          "input_path": {
            "type": "STRING"
          },
          "pipeline": {
            "type": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "data-ingestion-pipeline_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          },
          "model-evaluation-pipeline_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.12"
  },
  "runtimeConfig": {
    "parameters": {
      "dataset_version": {
        "intValue": "4"
      },
      "framework": {
        "stringValue": "Tensorflow"
      },
      "input_path": {
        "stringValue": "crazy-hippo-01.census_data_us.census_raw"
      },
      "pipeline": {
        "stringValue": "DL Version 8 (Tensorflow)"
      }
    }
  }
}