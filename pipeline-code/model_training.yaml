name: Model training
inputs:
- {name: TRAINING_DATA, type: Dataset}
- {name: TEST_DATA, type: Dataset}
- {name: VALIDATION_DATA, type: Dataset}
outputs:
- {name: MODEL, type: Model}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'pandas' 'pyarrow' 'gcsfs' 'google-cloud-bigquery-storage' 'tensorflow==2.5' 'db_dtypes' 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef model_training(TRAINING_DATA: Input[Dataset], \n         \
      \       TEST_DATA: Input[Dataset], \n                VALIDATION_DATA: Input[Dataset],\n\
      \                MODEL: Output[Model]\n               ):\n\n\n    import pandas\
      \ as pd\n    import time\n    import tensorflow as tf\n    from tensorflow import\
      \ keras\n    from tensorflow.keras import layers\n    from tensorflow.keras.layers.experimental\
      \ import preprocessing\n\n    #VARIABLES AND TRAINING PARAMETERS\n    TRAIN_DATA\
      \ = pd.read_csv(TRAINING_DATA.path)\n    TEST_DATA = pd.read_csv(TEST_DATA.path)\n\
      \    VAL_DATA = pd.read_csv(VALIDATION_DATA.path)\n\n    BATCH_SIZE = 32\n\n\
      \    print(tf.__version__)\n\n    print(MODEL.path)\n\n    #TENSORFLOW DATASET\
      \ FUNCTION\n    def helperfunc_create_dataset(dataframe, shuffle=True, batch_size=5):\n\
      \        dataframe = dataframe.copy()\n        labels = dataframe.pop('label')\n\
      \        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n\
      \        if shuffle:\n            ds = ds.shuffle(buffer_size=len(dataframe))\n\
      \        ds = ds.batch(batch_size)\n        ds = ds.prefetch(batch_size)\n \
      \       return ds\n\n    #NORMALIZATION FUNCTION\n    def helperfunc_get_normalization_layer(name,\
      \ dataset):\n        # Create a Normalization layer for our feature.\n     \
      \   normalizer = preprocessing.Normalization()\n\n        # Prepare a Dataset\
      \ that only yields our feature.\n        feature_ds = dataset.map(lambda x,\
      \ y: x[name])\n\n        # Learn the statistics of the data.\n        normalizer.adapt(feature_ds)\n\
      \n        return normalizer\n\n    #CATEGORY ENCODING FUNCTION\n    def helperfunc_get_category_encoding_layer(name,\
      \ dataset, dtype, max_tokens=None):\n        # Create a StringLookup layer which\
      \ will turn strings into integer indices\n        if dtype == 'string':\n  \
      \          index = preprocessing.StringLookup(max_tokens=max_tokens)\n     \
      \   else:\n            index = preprocessing.IntegerLookup(max_values=max_tokens)\n\
      \n        # Prepare a Dataset that only yields our feature\n        feature_ds\
      \ = dataset.map(lambda x, y: x[name])\n\n        # Learn the set of possible\
      \ values and assign them a fixed integer index.\n        index.adapt(feature_ds)\n\
      \n        # Create a Discretization for our integer indices.\n        encoder\
      \ = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n\n      \
      \  # Prepare a Dataset that only yields our feature.\n        feature_ds = feature_ds.map(index)\n\
      \n        # Learn the space of possible indices.\n        encoder.adapt(feature_ds)\n\
      \n        # Apply one-hot encoding to our indices. The lambda function captures\
      \ the\n        # layer so we can use them, or include them in the functional\
      \ model later.\n        return lambda feature: encoder(index(feature))\n\n \
      \   #CREATE TENSORFLOW DATASETS\n    TRAIN_DS = helperfunc_create_dataset(TRAIN_DATA,\
      \ batch_size=BATCH_SIZE)\n    VALIDATION_DS = helperfunc_create_dataset(VAL_DATA,\
      \ shuffle=False, batch_size=BATCH_SIZE)\n    TESTING_DS = helperfunc_create_dataset(TEST_DATA,\
      \ shuffle=False, batch_size=BATCH_SIZE)\n\n    #CREATE PREPROCESSING LAYERS\n\
      \    ALL_INPUTS = []\n    ENCODED_FEATURES = []\n\n    NUMERICAL = ['age' ,\
      \ 'capital_gain']\n    CATEGORICAL_INT_COLS = ['education_num']\n    CATEGORICAL_STRING_COLS\
      \ = ['occupation', \n                               'workclass', \n        \
      \                       'marital_status']\n    TARGET = ['label']\n\n    # Numeric\
      \ features.\n    for header in NUMERICAL:\n        numeric_col = tf.keras.Input(shape=(1,),\
      \ name=header)\n        normalization_layer = helperfunc_get_normalization_layer(header,\
      \ TRAIN_DS)\n        encoded_numeric_col = normalization_layer(numeric_col)\n\
      \        ALL_INPUTS.append(numeric_col)\n        ENCODED_FEATURES.append(encoded_numeric_col)\n\
      \n    # Categorical features encoded as integers.\n    for header in CATEGORICAL_INT_COLS:\n\
      \        categorical_int_col = tf.keras.Input(shape=(1,), name=header, dtype='int64')\n\
      \        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS,\
      \ dtype='int64', max_tokens=5)\n        encoded_categorical_int_col = encoding_layer(categorical_int_col)\n\
      \        ALL_INPUTS.append(categorical_int_col)\n        ENCODED_FEATURES.append(encoded_categorical_int_col)\n\
      \n    # Categorical features encoded as string.\n    for header in CATEGORICAL_STRING_COLS:\n\
      \        categorical_string_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n\
      \        encoding_layer = helperfunc_get_category_encoding_layer(header, TRAIN_DS,\
      \ dtype='string', max_tokens=5)\n        encoded_categorical_string_col = encoding_layer(categorical_string_col)\n\
      \        ALL_INPUTS.append(categorical_string_col)\n        ENCODED_FEATURES.append(encoded_categorical_string_col)\n\
      \n\n    #CREATE and COMPILE MODEL\n    all_features = tf.keras.layers.concatenate(ENCODED_FEATURES)\n\
      \    x = tf.keras.layers.Dense(32, activation=\"relu\")(all_features)\n    x\
      \ = tf.keras.layers.Dropout(0.5)(x)\n    output = tf.keras.layers.Dense(1)(x)\n\
      \    model = tf.keras.Model(ALL_INPUTS, output)\n    model.compile(optimizer='adam',\n\
      \                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n\
      \                  metrics=[\"accuracy\"])\n\n    #TRAIN MODEL\n    history\
      \ = model.fit(TRAIN_DS, epochs=10, validation_data=VALIDATION_DS)\n\n\n    #Define\
      \ Bucket in GCS for Model Storage\n    BUCKET = 'gs://crazy-pipelines/models/'\n\
      \n    #Define MODEL PATH \n    MODEL_PATH = BUCKET + 'earnings_model{}'.format(str(int(time.time())))\n\
      \n    MODEL.uri = MODEL_PATH \n\n\n    #Save model to Artifact Store for Project\n\
      \    model.save(MODEL.path)\n\n    print('Model saved to: ' + MODEL.path)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - model_training
